# Atomcamp-Project04
This project predicts the chance of admission for applicants using various machine learning regression models. It focuses on model building, optimization, and interpretation.

# ğŸ“ Graduate Admission Prediction â€“ Global Tech University (Regression Case Study)

## âœ… Final Case Study Summary

| Component                            | Details                                                                 |
|--------------------------------------|-------------------------------------------------------------------------|
| ğŸ“Š Dataset                           | Graduate Admission Data (400 records, 7 features + target)               
| ğŸ§¹ Data Cleaning                     | Verified no missing/null values. All columns cleaned and cast properly.  
| ğŸ“ˆ Baseline Model                    | Multiple Linear Regression (RÂ² = **0.82**, RMSE = **0.068**)             
| ğŸ” Multicollinearity Check           | Used **Variance Inflation Factor (VIF)** to detect collinearity.         
|   â®¡ Action Taken                      | No extreme multicollinearity found (VIF < 10), so all features retained. 
| ğŸŒ³ Decision Tree Regressor           | Initial RÂ² = **0.78**; prone to overfitting without tuning                
|     â®¡ Hyperparameter Tuning          | Used **GridSearchCV** to find best `max_depth`, `min_samples_split`, etc. 
|     â®¡ Optimized Performance          | RÂ² improved to **0.795**, RMSE = **0.073**                                
| ğŸŒ² Random Forest Regressor           | Initial RÂ² = **0.81**, RMSE = ~0.070                                    
|     â®¡ Hyperparameter Tuning          | Used **GridSearchCV** to search over `n_estimators`, `max_depth`, etc.  
|     â®¡  GridSearchCV                  | Accurate grid-based search is suitable due to the small feature space    
|     â®¡ Tuned Performance              | RÂ² = **0.81**, stable with reduced error and overfitting                 
| â­ Feature Selection (RF Importance)| Top 5: `CGPA`, `GRE Score`, `TOEFL Score`, `University Rating`, `SOP`     
| ğŸ” Re-trained Models on Top 5       | Linear (RÂ² = 0.798), DT (RÂ² = 0.778), RF (RÂ² = 0.794) â€“ still strong     
| ğŸ“ˆ Observation on Top 5 Features    | âœ… Simplified models with minimal accuracy loss; slightly reduced RÂ² but improved generalization and interpretability. Ideal for deployment scenarios.                      
| ğŸ“‰ Residual Analysis               | Random Forest residuals were evenly spread â†’ model generalizes well       
| ğŸ§ª Demo Applicant Test             | ğŸ“ Predicted Chance: 79.59% â†’ âŒ No Scholarship                         
| ğŸ“ Scholarship Rule Applied        | IF (Admit > 0.8 AND CGPA > 8.5 AND Research == 1) â†’ âœ… Scholarship       

---

## ğŸ“Œ Key Modeling Decisions and Justifications

- **ğŸ”¢ Multiple Linear Regression** was chosen as a baseline for interpretability. Coefficients helped identify key predictors.
- **ğŸ“‰ Multicollinearity** was tested using **VIF**. No predictors were dropped as all VIFs were below acceptable thresholds.
- **ğŸŒ³ Decision Tree** provided non-linear modeling but needed **hyperparameter tuning** to prevent overfitting.
- **ğŸŒ² Random Forest** was chosen for its ensemble strength and robust performance. GridSearchCV was preferred over RandomizedSearchCV due to a manageable search space and the need for deterministic best-fit parameters.
- **ğŸŒŸ Feature Selection** using feature importances from Random Forest helped simplify models while retaining strong accuracy.
- **ğŸ“Š Residual Plots** were used to verify random error distribution, confirming models are not biased across prediction ranges.

---

## ğŸ§  Tools & Libraries Used

- `pandas`, `numpy`, `seaborn`, `matplotlib`
- `sklearn.linear_model`, `sklearn.tree`, `sklearn.ensemble`
- `sklearn.model_selection` for train-test splits and hyperparameter tuning
- `sklearn.metrics` for evaluation

---

### ğŸ“Š Models Trained & Performance

| Model                      | RÂ² Score | RMSE      |
|---------------------------|----------|------------|
| Multiple Linear Regression | 0.82     | 0.0679    |
| Decision Tree Regressor    | 0.78     | -         |
| Decision Tree (Tuned)      | 0.795    | 0.0727    |
| Random Forest (Randomized) | 0.808    | -         |
| Random Forest (GridSearch) | 0.812    |  -        |
| Top 5 Features - Linear    | 0.798    | 0.0721    |
| Top 5 Features - DT    | 0.77    | 0.075    |
| Top 5 Features - RF        | 0.794    | 0.0728    |

## ğŸ“‚ Included in this Notebook

- Exploratory Data Analysis & Cleaning  
- Baseline Model + Coefficient Analysis  
- Multicollinearity Detection (VIF)  
- Model Comparison: Linear, Decision Tree, Random Forest  
- Hyperparameter Optimization (GridSearchCV)  
- Feature Selection & Reduced Modeling  
- Residual Error Analysis  
- Demo Prediction with Scholarship Logic  



